{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5 -- Human Detection \n",
    "This tutorial will show you how to detect human with the robot. We will use multi-thread to capture the color images and depth images. Then the ssd mobilenet v2 nerual network structure is used for human detection. To enable real-time performance, the tensorRT deep learning framework is used. More information can be found in the nvidia's jetbot project website: https://github.com/NVIDIA-AI-IOT/jetbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 load the pre-trained object detection model\n",
    "\n",
    "**Before running the program, we need to download the pre-trained detection model and put it in the current folder.** The model can be found in the following link:https://drive.google.com/file/d/1KjlDMRD8uhgQmQK-nC2CZGHFTbq4qQQH/view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "from tensorrt_model import TRTModel\n",
    "from ssd_tensorrt import load_plugins, parse_boxes,TRT_INPUT_NAME, TRT_OUTPUT_NAME\n",
    "import ctypes\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import ctypes\n",
    "    \n",
    "mean = 255.0 * np.array([0.5, 0.5, 0.5])\n",
    "stdev = 255.0 * np.array([0.5, 0.5, 0.5])\n",
    "\n",
    "def bgr8_to_ssd_input(camera_value):\n",
    "    x = camera_value\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1)).astype(np.float32)\n",
    "    x -= mean[:, None, None]\n",
    "    x /= stdev[:, None, None]\n",
    "    return x[None, ...]\n",
    "\n",
    "class ObjectDetector(object):\n",
    "    \n",
    "    def __init__(self, engine_path, preprocess_fn=bgr8_to_ssd_input):\n",
    "        logger = trt.Logger()\n",
    "        trt.init_libnvinfer_plugins(logger, '')\n",
    "        load_plugins()\n",
    "        self.trt_model = TRTModel(engine_path, input_names=[TRT_INPUT_NAME],output_names=[TRT_OUTPUT_NAME, TRT_OUTPUT_NAME + '_1'])\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        \n",
    "    def execute(self, *inputs):\n",
    "        trt_outputs = self.trt_model(self.preprocess_fn(*inputs))\n",
    "        return parse_boxes(trt_outputs)\n",
    "    \n",
    "    def __call__(self, *inputs):\n",
    "        return self.execute(*inputs)\n",
    "\n",
    "model = ObjectDetector('ssd_mobilenet_v2_coco.engine')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Initialize the camera instance for the Intel realsense sensor D435i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use traitlets and widgets to display the image in Jupyter Notebook\n",
    "import traitlets\n",
    "from traitlets.config.configurable import SingletonConfigurable\n",
    "\n",
    "#use opencv to covert the depth image to RGB image for displaying purpose\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#using realsense to capture the color and depth image\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "#multi-threading is used to capture the image in real time performance\n",
    "import threading\n",
    "\n",
    "class Camera(SingletonConfigurable):\n",
    "    \n",
    "    #this changing of this value will be captured by traitlets\n",
    "    color_value = traitlets.Any()\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Camera, self).__init__()\n",
    "        \n",
    "        #configure the color and depth sensor\n",
    "        self.pipeline = rs.pipeline()\n",
    "        self.configuration = rs.config()  \n",
    "        \n",
    "        #set resolution for the color camera\n",
    "        self.color_width = 640\n",
    "        self.color_height = 480\n",
    "        self.color_fps = 30\n",
    "        self.configuration.enable_stream(rs.stream.color, self.color_width, self.color_height, rs.format.bgr8, self.color_fps)\n",
    "\n",
    "        #set resolution for the depth camera\n",
    "        self.depth_width = 640\n",
    "        self.depth_height = 480\n",
    "        self.depth_fps = 30\n",
    "        self.configuration.enable_stream(rs.stream.depth, self.depth_width, self.depth_height, rs.format.z16, self.depth_fps)\n",
    "\n",
    "        #flag to control the thread\n",
    "        self.thread_runnning_flag = False\n",
    "        \n",
    "        #start the RGBD sensor\n",
    "        self.pipeline.start(self.configuration)\n",
    "        self.pipeline_started = True\n",
    "        frames = self.pipeline.wait_for_frames()\n",
    "\n",
    "        #start capture the first color image\n",
    "        color_frame = frames.get_color_frame()   \n",
    "        image = np.asanyarray(color_frame.get_data())\n",
    "        self.color_value = image\n",
    "\n",
    "        #start capture the first depth image\n",
    "        ### \"depth_image\" changed to an instance variable\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        self.depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(self.depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "        self.depth_value = depth_colormap   \n",
    "\n",
    "    def _capture_frames(self):\n",
    "        while(self.thread_runnning_flag==True): #continue until the thread_runnning_flag is set to be False\n",
    "            frames = self.pipeline.wait_for_frames() #receive data from RGBD sensor\n",
    "            \n",
    "            color_frame = frames.get_color_frame() #get the color image\n",
    "            image = np.asanyarray(color_frame.get_data()) #convert color image to numpy array\n",
    "            self.color_value = image #assign the numpy array image to the color_value variable \n",
    "\n",
    "            depth_frame = frames.get_depth_frame() #get the depth image           \n",
    "            self.depth_image = np.asanyarray(depth_frame.get_data()) #convert depth data to numpy array\n",
    "            #conver depth data to BGR image for displaying purpose\n",
    "            depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(self.depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "            self.depth_value = depth_colormap #assign the color BGR image to the depth value\n",
    "    \n",
    "    def start(self): #start the data capture thread\n",
    "        if self.thread_runnning_flag == False: #only process if no thread is running yet\n",
    "            self.thread_runnning_flag=True #flag to control the operation of the _capture_frames function\n",
    "            self.thread = threading.Thread(target=self._capture_frames) #link thread with the function\n",
    "            self.thread.start() #start the thread\n",
    "\n",
    "    def stop(self): #stop the data capture thread\n",
    "        if self.thread_runnning_flag == True:\n",
    "            self.thread_runnning_flag = False #exit the while loop in the _capture_frames\n",
    "            self.thread.join() #wait the exiting of the thread       \n",
    "\n",
    "def bgr8_to_jpeg(value):#convert numpy array to jpeg coded data for displaying \n",
    "    return bytes(cv2.imencode('.jpg',value)[1])\n",
    "\n",
    "#create a camera object\n",
    "camera = Camera.instance()\n",
    "camera.start() # start capturing the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Perform object detection and display the results on widgets\n",
    "Human is labeled is 1 in the pretrained model. A full list of the detection class indices can be found in the following link https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_complete_label_map.pbtxt\n",
    "\n",
    "The following program will ask the robot to move forward if a human is detected in the captured color image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from numpy import sign\n",
    "\n",
    "class RobotController:\n",
    "    def __init__(self):\n",
    "        ### SET DEFUALT INSTANCE VARIABLES ###\n",
    "        # self.d_list records data from previous detections\n",
    "        self.d_list = []\n",
    "        \n",
    "        # self.lag is the length of time to look for detection within, in case of noise\n",
    "        self.lag = 0.3\n",
    "        \n",
    "        # self.speed is the robot's speed\n",
    "        self.speed = 1\n",
    "        \n",
    "        '''               mid      \n",
    "        0  80  160  240   320  480  560  640\n",
    "           wmin ws              we  wmax\n",
    "        |   |   |    |     |    |   |   |\n",
    "        [---+---+-------------------+---]       0\n",
    "        |---+---+---------------+---+---|--hmin 60       \n",
    "        |---+---+---------------+---+---|--hs   120\n",
    "        |   |   |000000000000000|   |   |       180 \n",
    "        |   |   |000000000000000|   |   |--he   240 mid\n",
    "        |---+---+---------------+---+---|       300\n",
    "        |---+---+---------------+---+---|       360\n",
    "        [---+---+---------------+---+---]--hmax 420\n",
    "        [---+---+---------------+---+---]       480\n",
    "        '''\n",
    "        self.hmid = 240\n",
    "        self.wmid = 320\n",
    "\n",
    "        self.wmin = 80 \n",
    "        self.ws = 160\n",
    "        self.we = 480\n",
    "        self.wmax = 560\n",
    "\n",
    "        self.hmin = 60\n",
    "        self.hs = 120\n",
    "        self.he = 180 #self.hmid\n",
    "        self.hmax = self.hmid #360\n",
    "\n",
    "        self.height = 640\n",
    "        self.width = 480\n",
    "        \n",
    "        self.var = 50\n",
    "        \n",
    "    def SetVar(self, var):\n",
    "        self.var = var\n",
    "\n",
    "    \n",
    "    def getDiag(self, bbox):\n",
    "        return (((self.width*(bbox[2]-bbox[0]))**2+(self.height*(bbox[3]-bbox[1]))**2))**(1/2)\n",
    "\n",
    "    def getMagnitude(self, b1w, b1h, b2w, b2h):\n",
    "        return (((b2w-b1w)**2+(b2h-b1h)**2))**(1/2)\n",
    "    \n",
    "    def getDirection(self, b1w, b1h, b2w, b2h):\n",
    "        return (math.tanh((b2h-b1h)/(b2w-b2h)))**(-1)\n",
    "    \n",
    "    \n",
    "    def setTarget(self, tagetBox, currentBox):\n",
    "        pass\n",
    "    \n",
    "    def stop(self):\n",
    "        robot.stop()\n",
    "\n",
    "    \n",
    "    def follow(self, tagetBox, currentBox):\n",
    "        centerW=320\n",
    "        centerH=240\n",
    "        # Set coordinates\n",
    "        # target box\n",
    "        tw1 = self.width * tagetBox[0]\n",
    "        tw2 = self.width * tagetBox[2]\n",
    "\n",
    "        th1 = self.height * tagetBox[1]\n",
    "        th2 = self.height * tagetBox[3]\n",
    "\n",
    "        tcw= (tw1+((tw2-tw1)/2))\n",
    "        tch= (th1+((th2-th1)/2))\n",
    "\n",
    "        tMagnitude = self.getMagnitude(centerW,centerH,tcw,tch)\n",
    "        tDirection = self.getDirection(centerW,centerH,tcw,tch)\n",
    "        \"\"\"\n",
    "        tQuadrant=1\n",
    "        if tw1<320:\n",
    "            if th1<=240:\n",
    "                tQuadrant = 2\n",
    "            else:\n",
    "                tQuadrant = 3\n",
    "        else:   \n",
    "            if th1<=240:\n",
    "                tQuadrant = 1\n",
    "            else:\n",
    "                tQuadrant = 4 \"\"\" \n",
    "\n",
    "        # current box\n",
    "        cw1 = self.width * tagetBox[0]\n",
    "        cw2 = self.width * tagetBox[2]\n",
    "\n",
    "        ch1 = self.height * tagetBox[1]\n",
    "        ch2 = self.height * tagetBox[3]\n",
    "        \n",
    "        ccw= (cw1+((cw2-cw1)/2))\n",
    "        cch= (ch1+((ch2-ch1)/2))\n",
    "        cMagnitude = self.getMagnitude(centerW,centerH,ccw,cch)\n",
    "        cDirection = self.getDirection(centerW,centerH,ccw,cch)\n",
    "\n",
    "        cQuadrant=0\n",
    "        ch1+=self.var\n",
    "        if ccw<280:\n",
    "            if ch1<=180:\n",
    "                cQuadrant = 3\n",
    "            elif ch1>180 and ch1<300:\n",
    "                cQuadrant = 4\n",
    "            elif ch1>300:\n",
    "                cQuadrant = 5\n",
    "        elif ccw>=280 and ccw<=360:\n",
    "                if ch1<=180:\n",
    "                    cQuadrant = 2\n",
    "                elif ch1>180 and ch1<300:\n",
    "                    cQuadrant = 9\n",
    "                elif ch1>300:\n",
    "                    cQuadrant = 6 \n",
    "        elif ccw>360:\n",
    "                if ch1<=180:\n",
    "                    cQuadrant = 1\n",
    "                elif ch1>180 and ch1<300:\n",
    "                    cQuadrant = 8\n",
    "                elif ch1>300:\n",
    "                    cQuadrant = 7 \n",
    "\n",
    "        \"\"\"magnitude = self.getMagnitude(tcw,tch,ccw,cch)\n",
    "        direction = self.getDirection(tcw,tch,ccw,cch)\"\"\"\n",
    "        # Determine if the current box is further from the center than the last box\n",
    "        further=True\n",
    "        if cMagnitude>tMagnitude:\n",
    "            further=True\n",
    "        else:\n",
    "            further=False\n",
    "\n",
    "        # Determine if the current box is further from the center than the last box\n",
    "        speed=0.0\n",
    "        minThreshold=20\n",
    "        if cMagnitude>minThreshold:\n",
    "            speed=0.3\n",
    "        else:\n",
    "            speed=0.5\n",
    "        \n",
    "        # forward backward\n",
    "        # backward\n",
    "        print (\"quadrant\", cQuadrant)\n",
    "        if cQuadrant==1:\n",
    "            # lostTarget = 1\n",
    "            # move right -forward \n",
    "            speed=0.8\n",
    "            robot.backward_left(speed)\n",
    "            return\n",
    "        \n",
    "        if cQuadrant==2:\n",
    "            # lostTarget = 1\n",
    "            # move left-forward  \n",
    "            speed=0.7\n",
    "            robot.backward(speed)\n",
    "            return\n",
    "\n",
    "        if cQuadrant==3:\n",
    "            # lostTarget = 1\n",
    "            # move left-forwad \n",
    "            speed=0.8\n",
    "            robot.backward_right(speed)\n",
    "            return\n",
    "        \n",
    "        if cQuadrant==4:\n",
    "            # lostTarget = 1\n",
    "            # move right\n",
    "            speed=0.65\n",
    "            robot.left(speed)\n",
    "            return\n",
    "        if cQuadrant==5:\n",
    "            # lostTarget = 1\n",
    "            # move backward_left\n",
    "            speed=0.8\n",
    "            robot.forward_right(speed)\n",
    "            return\n",
    "        if cQuadrant==6:\n",
    "            # lostTarget = 1\n",
    "            # move backward\n",
    "            speed=0.7\n",
    "            robot.forward(speed)\n",
    "            return\n",
    "        if cQuadrant==7:\n",
    "            # lostTarget = 1\n",
    "            # move right-forward \n",
    "            speed=0.8\n",
    "            robot.forward_left(speed)\n",
    "            return\n",
    "        if cQuadrant==8:\n",
    "            # lostTarget = 1\n",
    "            # move right-forward \n",
    "            speed=0.65\n",
    "            robot.right(speed)\n",
    "            return\n",
    "            \n",
    "        robot.stop()\n",
    "        return\n",
    "            \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "width = 640\n",
    "height = 480\n",
    "\n",
    "image_widget = widgets.Image(format='jpeg', width=640, height=480)\n",
    "label_widget = widgets.IntText(value=1, description='tracked label')\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([image_widget,]),\n",
    "    label_widget\n",
    "]))\n",
    "\n",
    "import time\n",
    "from RobotClass import Robot\n",
    "\n",
    "#initialize the Robot class\n",
    "robot = Robot()\n",
    "\n",
    "### Initialize the Robot Controller class\n",
    "rc = RobotController()\n",
    "\n",
    "#\n",
    "# set initialized to false\n",
    "initialized = False\n",
    "\n",
    "# set initial coordinates to zero\n",
    "tagetBox = None\n",
    "tagetBoxDiag =0\n",
    "\n",
    "def processing(change):\n",
    "    global tagetBox,rc,tagetBoxDiag\n",
    "    image = change['new']\n",
    "     \n",
    "    imgsized= cv2.resize(image,(300,300))\n",
    "    # compute all detected objects\n",
    "    detections = model(imgsized)\n",
    "    \n",
    "    matching_detections = [d for d in detections[0] if d['label'] == int(label_widget.value)]\n",
    "    \n",
    "    # reset biggest box coordinates\n",
    "    biggestBox = None\n",
    "    biggestBoxDiag = 5\n",
    "\n",
    "    for det in matching_detections:\n",
    "        bbox = det['bbox']\n",
    "        if bbox[0] != None:\n",
    "            diag = rc.getDiag(bbox)\n",
    "        else:\n",
    "            diag = 0\n",
    "        if (diag > biggestBoxDiag):\n",
    "            biggestBox = bbox\n",
    "            biggestBoxDiag = diag\n",
    "            if tagetBox ==None:\n",
    "                tagetBox = biggestBox\n",
    "\n",
    "    \n",
    "    ### Pass detection to Robot Controller\n",
    "    if biggestBoxDiag >10:\n",
    "        rc.follow(tagetBox,biggestBox )\n",
    "        tagetBox = biggestBox\n",
    "        tagetBoxDiag = biggestBoxDiag\n",
    "\n",
    "    else:\n",
    "        if tagetBoxDiag >20:\n",
    "            rc.follow(tagetBox,tagetBox )\n",
    "        if biggestBoxDiag > tagetBoxDiag-20 or biggestBoxDiag< tagetBoxDiag+20:\n",
    "            tagetBox = biggestBox\n",
    "            tagetBoxDiag = biggestBoxDiag\n",
    "        rc.stop()\n",
    "\n",
    "    \n",
    "    for det in matching_detections:\n",
    "        bbox = det['bbox']\n",
    "        #cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (255, 0, 0), 2)\n",
    "        \n",
    "    image_widget.value = bgr8_to_jpeg(image)\n",
    "    \n",
    "#the camera.observe function will monitor the color_value variable. If this value changes, the excecute function will be excuted.\n",
    "camera.observe(processing, names='color_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rc.SetVar(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "### 1. Please try to calculate the distance between the detected human and the robot using the depth image. (Note: You can refer to tutorial 2 to get the depth information for a specific point)\n",
    "### 2. Please try to add collision avoidance function into this program to protect the robot. (Note: You can refer to tutorial 4 for the collision avoidance)\n",
    "### 3. Think about how to control the robot to move towards the detected human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
